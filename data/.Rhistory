# LU solve for pagerank
pgrank <- solve(M, c(1, rep(0, n - 1)))
# LU method output
return(pgrank)
}
LU_speed <- system.time(LU_method <- compute_pagerank_lu(Acleaned))
LU_speed
top20_lu <- order(LU_method, decreasing = TRUE)[1:20]
toplist <- cbind(Ucleaned[top20_lu], LU_method[top20_lu],
in_degrees[top20_lu], out_degrees[top20_lu])
colnames(toplist) <- c("URL", "PageRank (LU)", "In-Deg", "Out-Deg")
kable(toplist, format = "pandoc", caption = "PageRank by LU",
align = c("l", "l", "l", "l"), row.names = FALSE)
compute_pagerank_lu <- function(A, teleportation_parameter = 0.85) {
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P^t
M <- - t(G + z)
diag(M) <- diag(M) + 1
# sum-to-1 constraint in first row of M
M[1, ] <- 1
# LU solve for pagerank
pgrank <- solve(M, c(1, rep(0, n - 1)))
# LU method output
return(pgrank)
}
LU_speed <- system.time(LU_method <- compute_pagerank_lu(Acleaned))
LU_speed
top20_lu <- order(LU_method, decreasing = TRUE)[1:20]
toplist <- cbind(Ucleaned[top20_lu], LU_method[top20_lu],
in_degrees[top20_lu], out_degrees[top20_lu])
colnames(toplist) <- c("URL", "PageRank (LU)", "In-Deg", "Out-Deg")
library(knitr)
kable(toplist, format = "pandoc", caption = "PageRank by LU",
align = c("l", "l", "l", "l"), row.names = FALSE)
compute_pagerank_lu <- function(A, teleportation_parameter = 0.85) {
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P^t
M <- - t(G + z)
diag(M) <- diag(M) + 1
# sum-to-1 constraint in first row of M
M[1, ] <- 1
# LU solve for pagerank
pgrank <- solve(M, c(1, rep(0, n - 1)))
# LU method output
return(pgrank)
}
LU_speed <- system.time(LU_method <- compute_pagerank_lu(Acleaned))
LU_speed
top20_lu <- order(LU_method, decreasing = TRUE)[1:20]
toplist <- cbind(Ucleaned[top20_lu], LU_method[top20_lu],
in_degrees[top20_lu], out_degrees[top20_lu])
colnames(toplist) <- c("URL", "PageRank (LU)", "In-Deg", "Out-Deg")
library(knitr)
kable(toplist,
format = "pandoc",
caption = "PageRank by LU",
# align = c("l", "l", "l", "l"),
row.names = TRUE)
compute_pagerank_lu <- function(A, teleportation_parameter = 0.85) {
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P^t
M <- - t(G + z)
diag(M) <- diag(M) + 1
# sum-to-1 constraint in first row of M
M[1, ] <- 1
# LU solve for pagerank
pgrank <- solve(M, c(1, rep(0, n - 1)))
# LU method output
return(pgrank)
}
LU_speed <- system.time(LU_method <- compute_pagerank_lu(Acleaned))
LU_speed
top20_lu <- order(LU_method, decreasing = TRUE)[1:20]
toplist <- cbind(Ucleaned[top20_lu], LU_method[top20_lu],
in_degrees[top20_lu], out_degrees[top20_lu])
colnames(toplist) <- c("URL", "PageRank (LU)", "In-Deg", "Out-Deg")
library(knitr)
kable(toplist,
format = "pandoc",
caption = "PageRank by LU",
align = c("l", "l", "l", "l"),
row.names = FALSE)
# Read A.txt; I will transpose A later as instructed.
At <- read.table("A.txt", sep = ",")
# Set A as a matrix
At <- as.matrix(At)
# Take the transpose of A, as instructed
A <- t(At)
# Read U.txt
U <- read.table("U.txt", sep = " ")
# U has only one column, so we can extract the entire dataframe
# into a character vector to make it easier to work with
U <- U$V1
# Each page is represented by a single row, so the number
# of rows in `A` gives the number of total pages.
# dim(A) gives the dimensions of matrix A
# and the first entry gives the number of rows.
dim(A)[1]
# The number of edges or page links is the total number
# of connections between pages in the matrix.
sum(A)
# The number of dangling nodes is the number of pages with no out links
# i.e. the number of pages that lead to zero further pages; "dead end" pages.
# Counting each page's out links.
outlinks <- rowSums(A)
# Counting number of dead ends
sum(outlinks == 0)
# The max in-degree is the page with the most pages leading to it.
# Counting each page's in-links
inlinks <- colSums(A)
# Finding the page with the most in-links
max_indegree <- which.max(inlinks)
# Identifying the page with the most in-links
U[max_indegree]
# The max out-degree is the page leading to the most different other pages.
# Finding the page with the most out-links
out_degrees <- which.max(outlinks)
# Identifying the page with the most out-links
U[out_degrees]
image(1:500, 1:500, A, col = c("white", "black"), xlab = "", ylab = "")
# Visually indexed each URL to filter for invalid/strange URLs
weird_urls <- c(2, 46, 69, 150, 323, 382, 471)
# Creating a new U vector without the odd URLs
Ucleaned <- U[-weird_urls]
# Creating a new A matrix without the odd URLs in both rows and columns
Acleaned <- A[-weird_urls, -weird_urls]
compute_pagerank_qr <- function(A, teleportation_parameter = 0.85) {
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P
M <- - (G + z)
diag(M) <- diag(M) + 1
qrM <- qr(M)
pgrank <- qr.qy(qrM, c(rep(0, n - 1), 1))
pgrank <- abs(pgrank / sum(pgrank))
# QR method output
return(pgrank)
}
compute_pagerank_lu <- function(A, teleportation_parameter = 0.85) {
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P^t
M <- - t(G + z)
diag(M) <- diag(M) + 1
# sum-to-1 constraint in first row of M
M[1, ] <- 1
# LU solve for pagerank
pgrank <- solve(M, c(1, rep(0, n - 1)))
# LU method output
return(pgrank)
}
LU_speed <- system.time(LU_method <- compute_pagerank_lu(Acleaned))
LU_speed
top20_lu <- order(LU_method, decreasing = TRUE)[1:20]
toplist <- cbind(Ucleaned[top20_lu], LU_method[top20_lu],
in_degrees[top20_lu], out_degrees[top20_lu])
colnames(toplist) <- c("URL", "PageRank (LU)", "In-Deg", "Out-Deg")
library(knitr)
kable(toplist,
format = "pandoc",
caption = "PageRank by LU",
align = c("l", "l", "l", "l"),
row.names = FALSE)
compute_pagerank_lu <- function(A, teleportation_parameter = 0.85) {
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P^t
M <- - t(G + z)
diag(M) <- diag(M) + 1
# sum-to-1 constraint in first row of M
M[1, ] <- 1
# LU solve for pagerank
pgrank <- solve(M, c(1, rep(0, n - 1)))
# LU method output
return(pgrank)
}
LU_speed <- system.time(LU_method <- compute_pagerank_lu(Acleaned))
LU_speed
top20_lu <- order(LU_method, decreasing = TRUE)[1:20]
toplist <- cbind(Ucleaned[top20_lu], LU_method[top20_lu],
inlinks[top20_lu], outlinks[top20_lu])
colnames(toplist) <- c("URL", "PageRank (LU)", "In-Deg", "Out-Deg")
library(knitr)
kable(toplist,
format = "pandoc",
caption = "PageRank by LU",
align = c("l", "l", "l", "l"),
row.names = FALSE)
View(toplist)
compute_pagerank_lu <- function(A, teleportation_parameter = 0.85) {
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P^t
M <- - t(G + z)
diag(M) <- diag(M) + 1
# sum-to-1 constraint in first row of M
M[1, ] <- 1
# LU solve for pagerank
pgrank <- solve(M, c(1, rep(0, n - 1)))
# LU method output
return(pgrank)
}
LU_speed <- system.time(LU_method <- compute_pagerank_lu(Acleaned))
LU_speed
top20_lu <- order(LU_method, decreasing = TRUE)[1:20]
top20_df <- cbind(Ucleaned[top20_lu], LU_method[top20_lu], inlinks[top20_lu], outlinks[top20_lu])
colnames(top20_df) <- c("URL", "PageRank (LU)", "In-Deg", "Out-Deg")
library(knitr)
kable(toplist,
format = "pandoc",
caption = "PageRank by LU",
align = c("l", "l", "l", "l"),
row.names = FALSE)
compute_pagerank_qr <- function(A, teleportation_parameter = 0.85) {
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P
M <- - (G + z)
diag(M) <- diag(M) + 1
qrM <- qr(M)
pgrank <- qr.qy(qrM, c(rep(0, n - 1), 1))
pgrank <- abs(pgrank / sum(pgrank))
# QR method output
return(pgrank)
}
QR_comp_time <- system.time(QR_method <- compute_pagerank_qr(Acleaned))
QR_comp_time
top20_qr <- order(LU_method, decreasing = TRUE)[1:20]
top20_qr_df <- cbind(Ucleaned[top20_qr], QR_method[top20_qr], inlinks[top20_qr], outlinks[top20_qr])
colnames(top20_qr_df) <- c("URL", "PageRank (QR)", "In-Deg", "Out-Deg")
library(knitr)
kable(top20_qr_df,
format = "pandoc",
caption = "PageRank by LU",
align = c("l", "l", "l", "l"),
row.names = FALSE)
# Read A.txt; I will transpose A later as instructed.
At <- read.table("A.txt", sep = ",")
# Set A as a matrix
At <- as.matrix(At)
# Take the transpose of A, as instructed
A <- t(At)
# Read U.txt
U <- read.table("U.txt", sep = " ")
# U has only one column, so we can extract the entire dataframe
# into a character vector to make it easier to work with
U <- U$V1
# Each page is represented by a single row, so the number
# of rows in `A` gives the number of total pages.
# dim(A) gives the dimensions of matrix A
# and the first entry gives the number of rows.
dim(A)[1]
# The number of edges or page links is the total number
# of connections between pages in the matrix.
sum(A)
# The number of dangling nodes is the number of pages with no out links
# i.e. the number of pages that lead to zero further pages; "dead end" pages.
# Counting each page's out links.
outlinks <- rowSums(A)
# Counting number of dead ends
sum(outlinks == 0)
# The max in-degree is the page with the most pages leading to it.
# Counting each page's in-links
inlinks <- colSums(A)
# Finding the page with the most in-links
max_indegree <- which.max(inlinks)
# Identifying the page with the most in-links
U[max_indegree]
# The max out-degree is the page leading to the most different other pages.
# Finding the page with the most out-links
out_degrees <- which.max(outlinks)
# Identifying the page with the most out-links
U[out_degrees]
image(1:500, 1:500, A, col = c("white", "black"), xlab = "", ylab = "")
# Visually indexed each URL to filter for invalid/strange URLs
weird_urls <- c(2, 46, 69, 150, 323, 382, 471)
# Creating a new U vector without the odd URLs
Ucleaned <- U[-weird_urls]
# Creating a new A matrix without the odd URLs in both rows and columns
Acleaned <- A[-weird_urls, -weird_urls]
library(Matrix)
compute_pagerank_qr <- function(A, teleportation_parameter = 0.85) {
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P
M <- - (G + z)
diag(M) <- diag(M) + 1
qrM <- qr(M)
pgrank <- qr.qy(qrM, c(rep(0, n - 1), 1))
pgrank <- abs(pgrank / sum(pgrank))
# QR method output
return(pgrank)
}
QR_comp_time <- system.time(QR_method <- compute_pagerank_qr(Acleaned))
QR_comp_time
top20_qr <- order(LU_method, decreasing = TRUE)[1:20]
top20_qr_df <- cbind(Ucleaned[top20_qr], QR_method[top20_qr], inlinks[top20_qr], outlinks[top20_qr])
colnames(top20_qr_df) <- c("URL", "PageRank (QR)", "Page In-Degree", "Page Out-Degree")
library(knitr)
kable(top20_qr_df,
format = "pandoc",
caption = "PageRank by LU",
align = c("l", "l", "l", "l"),
row.names = FALSE)
compute_pagerank_lu <- function(A, teleportation_parameter = 0.85) {
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P^t
M <- - t(G + z)
diag(M) <- diag(M) + 1
# sum-to-1 constraint in first row of M
M[1, ] <- 1
# LU solve for pagerank
pgrank <- solve(M, c(1, rep(0, n - 1)))
# LU method output
return(pgrank)
}
LU_comp_time <- system.time(LU_method <- compute_pagerank_lu(Acleaned))
LU_comp_time
top20_lu <- order(LU_method, decreasing = TRUE)[1:20]
top20_lu_df <- cbind(Ucleaned[top20_lu], LU_method[top20_lu], inlinks[top20_lu], outlinks[top20_lu])
colnames(top20_lu_df) <- c("URL", "PageRank (LU)", "Page In-Degree", "Page Out-Degree")
library(knitr)
kable(top20_lu_df,
format = "pandoc",
caption = "PageRank by LU",
align = c("l", "l", "l", "l"),
row.names = FALSE)
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
teleportation_parameter = 0.85
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P^t
M <- - t(G + z)
diag(M) <- diag(M) + 1
# sum-to-1 constraint in first row of M
M[1, ] <- 1
# LU solve for pagerank
pgrank <- solve(M, c(1, rep(0, n - 1)))
# LU method output
return(pgrank)
n <- dim(A)[1]
A_sparse <- Matrix(A, sparse = TRUE)
out_degree <- rowSums(A_sparse)
out_degree_inv <- ifelse(out_degree > 0, teleportation_parameter / out_degree, 0)
z <- ifelse(out_degree > 0, 1 - teleportation_parameter, 1) / n
G <- out_degree_inv * A_sparse
# We have linear system M = I - P^t
M <- - t(G + z)
diag(M) <- diag(M) + 1
# sum-to-1 constraint in first row of M
M[1, ] <- 1
# LU solve for pagerank
pgrank <- solve(M, c(1, rep(0, n - 1)))
pgrank
M
M[1,]
M <- - t(G + z)
M[1,]
player_analysis_question <- read.csv("C:/Users/kobyl/Downloads/player_analysis_question.csv")
View(player_analysis_question)
player_analysis_question <- read.csv("C:/Users/kobyl/Downloads/player_analysis_question.csv")
View(player_analysis_question)
df <- read.csv("C:/Users/kobyl/Downloads/player_analysis_question.csv")
df$OBP <- (df$H + df$BB) / (df$AB + df$BB)
df
round(df$OBP, 3)
df$OBP = round(df$OBP, 3)
df$OBP
df$SLG = (df$H - df$X2B - df$X3B - df$HR) + 2*df$X2B + 3*df$X3B + 4*df$HR
df$SLG
df$SLG = ((df$H - df$X2B - df$X3B - df$HR) + 2*df$X2B + 3*df$X3B + 4*df$HR)/df$AB
df$SLG
df$ISO = df$SLG - df$BA
df$SLG = round(df$SLG, 3)
df
df$ISO = df$SLG - df$AVG
View(df)
setwd("C:/Users/kobyl/Desktop/Winter Quarter 2023 Resources/Advanced Statistical Computing (STA 141C)/NBA")
dataStats <- read.csv("all_seasons.csv")
setwd("C:/Users/kobyl/Desktop/Winter Quarter 2023 Resources/Advanced Statistical Computing (STA 141C)/NBA/data")
dataStats <- read.csv("all_seasons.csv")
setwd("C:/Users/kobyl/Desktop/Winter Quarter 2023 Resources/Advanced Statistical Computing (STA 141C)/NBA/data")
all_seasons <- read.csv("C:/Users/kobyl/Desktop/Winter Quarter 2023 Resources/Advanced Statistical Computing (STA 141C)/NBA/data/all_seasons.csv")
View(all_seasons)
dataStats <- all_seasons <- read.csv("data/all_seasons.csv")
dataStats <- all_seasons <- read.csv("data/all_seasons.csv")
dataStats <- dataStats[-1]
dataStats <- dataStats[-2]
dataStats <- dataStats[, -c(5:6)]
dataStats <- dataStats[-18]
# filter out undrafted players
nba_data <- dataStats[dataStats$draft_number != "Undrafted",]
players_data <- read.csv("all_seasons.csv")
dataStats <- read.csv("data/all_seasons.csv")
dataStats <- dataStats[-1]
dataStats <- dataStats[-2]
dataStats <- dataStats[, -c(5:6)]
dataStats <- dataStats[-18]
# filter out undrafted players
nba_data <- dataStats[dataStats$draft_number != "Undrafted",]
players_data <- read.csv("all_seasons.csv")
players_data <- read.csv("data/all_seasons.csv")
players_data <- read.csv("data/all_seasons.csv")
# Remove undrafted
players_data <- players_data[!(players_data$draft_year == "Undrafted"),]
players_data <- players_data[!(players_data$draft_round == "Undrafted"),]
players_data <- players_data[!(players_data$draft_number == "Undrafted"),]
# Make numeric
players_data$draft_year <- as.numeric(players_data$draft_year)
players_data$draft_round <- as.numeric(players_data$draft_round)
players_data$draft_number <- as.numeric(players_data$draft_number)
# Drop wrong data
players_data <- players_data[players_data$draft_year >= 1990,]
players_data <- players_data[players_data$draft_round <= 2,]
players_data <- players_data[players_data$draft_number <= 60,]
players_data <- players_data[players_data$draft_number >= 1,]
#Set data for draft number
nba_data <- players_data[, c(4, 5, 6, 11, 12, 13:21)]
#X = nba_data[,2:4]
#y = nba_data[,1]
# convert draft number and player height to numeric variables
nba_data$draft_number <- as.numeric(nba_data$draft_number)
nba_data$player_weight <- as.numeric(nba_data$player_weight)
nba_data$age <- as.numeric(nba_data$age)
# fit a linear regression model for points
lmPoints <- lm(pts ~ draft_number + player_height + age + player_weight + gp + net_rating + oreb_pct + dreb_pct + usg_pct + ts_pct + ast_pct, data = nba_data)
# predict points for draft picks 1-60
draft_picks <- data.frame(draft_number = 1:60, player_height = mean(nba_data$player_height), age = mean(nba_data$age), player_weight = mean(nba_data$player_weight), gp = mean(nba_data$gp), net_rating = mean(nba_data$net_rating), oreb_pct = mean(nba_data$oreb_pct), dreb_pct = mean(nba_data$dreb_pct), usg_pct = mean(nba_data$usg_pct), ts_pct = mean(nba_data$ts_pct), ast_pct = mean(nba_data$ast_pct))
pred_pts <- predict(lmPoints, newdata = draft_picks)
# fit a linear regression model for rebounds
lmRebounds <- lm(reb ~ draft_number + player_height + age + player_weight + gp + net_rating + oreb_pct + dreb_pct + usg_pct + ts_pct + ast_pct, data = nba_data)
# predict rebounds for draft picks 1-60
pred_reb <- predict(lmRebounds, newdata = draft_picks)
# fit a linear regression model for assists
lmAssists <- lm(ast ~ draft_number + player_height + age + player_weight + gp + net_rating + oreb_pct + dreb_pct + usg_pct + ts_pct + ast_pct, data = nba_data)
# predict assists for draft picks 1-60
pred_ast <- predict(lmAssists, newdata = draft_picks)
print(pred_pts)
print(pred_ast)
print(pred_reb)
# create a data frame with predicted values
pred_df <- data.frame(Draft_Pick = 1:60, Points = pred_pts, Assists = pred_ast, Rebounds = pred_reb)
# plot predicted values for each category by draft pick
library(ggplot2)
ggplot(pred_df, aes(x = Draft_Pick)) +
geom_point(aes(y = Points, color = "Points")) +
geom_point(aes(y = Assists, color = "Assists")) +
geom_point(aes(y = Rebounds, color = "Rebounds")) +
scale_color_manual(values = c("red", "green", "blue")) +
xlab("Draft Position") + ylab("Predicted Statistics") +
ggtitle("Predicted NBA Statistics based on Draft Position") +
labs(color = "Category")
install.packages("shiny")
install.packages("shiny")
install.packages("shiny")
install.packages("shiny")
library(shiny)
library(htmltools)
library(shiny)
install.packages("htmltools")
install.packages("htmltools")
